{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2a21b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "import functions as func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3613d9ef",
   "metadata": {},
   "source": [
    "Custom Funtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4162e858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/manohar/Documents/GCP_ML_Engineer/Code/Lab/9_Image_Ergonomics/images/image_7.jpg: 384x640 2 persons, 145.7ms\n",
      "Speed: 1.0ms preprocess, 145.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model=YOLO('models/yolo11l-pose.pt')\n",
    "\n",
    "predict=model(source='images/image_7.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e02604b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ultralytics.engine.results.Keypoints object with attributes:\n",
       "\n",
       "conf: tensor([[0.9905, 0.5967, 0.9964, 0.0183, 0.9907, 0.8593, 0.9829, 0.2782, 0.9577, 0.3855, 0.9608, 0.0701, 0.1545, 0.0062, 0.0169, 0.0017, 0.0026],\n",
       "        [0.9907, 0.9126, 0.9866, 0.3312, 0.9780, 0.9972, 0.9876, 0.9719, 0.8818, 0.8879, 0.7612, 0.9474, 0.9067, 0.1120, 0.0654, 0.0044, 0.0027]])\n",
       "data: tensor([[[5.1684e+02, 4.1999e+02, 9.9049e-01],\n",
       "         [5.1512e+02, 3.6402e+02, 5.9674e-01],\n",
       "         [4.6156e+02, 3.7569e+02, 9.9645e-01],\n",
       "         [4.4690e+02, 3.6348e+02, 1.8322e-02],\n",
       "         [2.9939e+02, 3.9711e+02, 9.9070e-01],\n",
       "         [3.8388e+02, 6.2787e+02, 8.5929e-01],\n",
       "         [2.5503e+02, 6.6315e+02, 9.8289e-01],\n",
       "         [6.7762e+02, 9.2908e+02, 2.7825e-01],\n",
       "         [7.0771e+02, 9.3964e+02, 9.5767e-01],\n",
       "         [1.1399e+03, 9.6028e+02, 3.8553e-01],\n",
       "         [1.1960e+03, 8.9946e+02, 9.6076e-01],\n",
       "         [3.5704e+02, 1.1174e+03, 7.0073e-02],\n",
       "         [2.4896e+02, 1.1250e+03, 1.5449e-01],\n",
       "         [6.8758e+02, 9.2507e+02, 6.2432e-03],\n",
       "         [5.2749e+02, 9.8209e+02, 1.6877e-02],\n",
       "         [7.6218e+02, 8.8107e+02, 1.7145e-03],\n",
       "         [6.9080e+02, 8.3490e+02, 2.5858e-03]],\n",
       "\n",
       "        [[1.0575e+03, 2.7093e+02, 9.9072e-01],\n",
       "         [1.0732e+03, 2.2333e+02, 9.1263e-01],\n",
       "         [1.0019e+03, 2.4595e+02, 9.8660e-01],\n",
       "         [1.0952e+03, 2.1024e+02, 3.3121e-01],\n",
       "         [9.2032e+02, 2.5923e+02, 9.7801e-01],\n",
       "         [1.2203e+03, 3.7069e+02, 9.9721e-01],\n",
       "         [8.1464e+02, 3.7462e+02, 9.8756e-01],\n",
       "         [1.2659e+03, 7.1401e+02, 9.7194e-01],\n",
       "         [8.0158e+02, 7.0163e+02, 8.8179e-01],\n",
       "         [1.3040e+03, 9.6090e+02, 8.8789e-01],\n",
       "         [8.9827e+02, 9.1785e+02, 7.6121e-01],\n",
       "         [1.1721e+03, 9.1384e+02, 9.4735e-01],\n",
       "         [9.0433e+02, 9.1469e+02, 9.0669e-01],\n",
       "         [1.2434e+03, 1.1250e+03, 1.1203e-01],\n",
       "         [9.3268e+02, 1.1250e+03, 6.5372e-02],\n",
       "         [1.1352e+03, 1.1250e+03, 4.3540e-03],\n",
       "         [1.1037e+03, 1.0944e+03, 2.7101e-03]]])\n",
       "has_visible: True\n",
       "orig_shape: (1125, 2000)\n",
       "shape: torch.Size([2, 17, 3])\n",
       "xy: tensor([[[ 516.8359,  419.9872],\n",
       "         [ 515.1242,  364.0210],\n",
       "         [ 461.5592,  375.6887],\n",
       "         [ 446.9029,  363.4763],\n",
       "         [ 299.3900,  397.1128],\n",
       "         [ 383.8797,  627.8724],\n",
       "         [ 255.0324,  663.1500],\n",
       "         [ 677.6215,  929.0845],\n",
       "         [ 707.7065,  939.6421],\n",
       "         [1139.8556,  960.2827],\n",
       "         [1195.9519,  899.4601],\n",
       "         [ 357.0352, 1117.3871],\n",
       "         [ 248.9610, 1125.0000],\n",
       "         [ 687.5777,  925.0740],\n",
       "         [ 527.4869,  982.0939],\n",
       "         [ 762.1812,  881.0677],\n",
       "         [ 690.8021,  834.9003]],\n",
       "\n",
       "        [[1057.5155,  270.9321],\n",
       "         [1073.2329,  223.3272],\n",
       "         [1001.9244,  245.9507],\n",
       "         [1095.2483,  210.2381],\n",
       "         [ 920.3234,  259.2266],\n",
       "         [1220.3232,  370.6936],\n",
       "         [ 814.6379,  374.6205],\n",
       "         [1265.8962,  714.0126],\n",
       "         [ 801.5815,  701.6274],\n",
       "         [1303.9811,  960.8956],\n",
       "         [ 898.2686,  917.8461],\n",
       "         [1172.1301,  913.8398],\n",
       "         [ 904.3300,  914.6946],\n",
       "         [1243.3806, 1125.0000],\n",
       "         [ 932.6804, 1125.0000],\n",
       "         [1135.2036, 1125.0000],\n",
       "         [1103.6766, 1094.3533]]])\n",
       "xyn: tensor([[[0.2584, 0.3733],\n",
       "         [0.2576, 0.3236],\n",
       "         [0.2308, 0.3339],\n",
       "         [0.2235, 0.3231],\n",
       "         [0.1497, 0.3530],\n",
       "         [0.1919, 0.5581],\n",
       "         [0.1275, 0.5895],\n",
       "         [0.3388, 0.8259],\n",
       "         [0.3539, 0.8352],\n",
       "         [0.5699, 0.8536],\n",
       "         [0.5980, 0.7995],\n",
       "         [0.1785, 0.9932],\n",
       "         [0.1245, 1.0000],\n",
       "         [0.3438, 0.8223],\n",
       "         [0.2637, 0.8730],\n",
       "         [0.3811, 0.7832],\n",
       "         [0.3454, 0.7421]],\n",
       "\n",
       "        [[0.5288, 0.2408],\n",
       "         [0.5366, 0.1985],\n",
       "         [0.5010, 0.2186],\n",
       "         [0.5476, 0.1869],\n",
       "         [0.4602, 0.2304],\n",
       "         [0.6102, 0.3295],\n",
       "         [0.4073, 0.3330],\n",
       "         [0.6329, 0.6347],\n",
       "         [0.4008, 0.6237],\n",
       "         [0.6520, 0.8541],\n",
       "         [0.4491, 0.8159],\n",
       "         [0.5861, 0.8123],\n",
       "         [0.4522, 0.8131],\n",
       "         [0.6217, 1.0000],\n",
       "         [0.4663, 1.0000],\n",
       "         [0.5676, 1.0000],\n",
       "         [0.5518, 0.9728]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict[0].keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38b1d2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_frame=predict[0].plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8737bb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For each detected person:\n",
    "\n",
    "if predict[0].keypoints is not None:\n",
    "    for kpts in predict[0].keypoints.data:\n",
    "        # Extract points (in format x,y,score)\n",
    "        kpts = kpts.cpu().numpy()\n",
    "\n",
    "        LEFT_SHOULDER = kpts[5][:2]\n",
    "        LEFT_ELBOW = kpts[7][:2]\n",
    "        LEFT_WRIST = kpts[9][:2]\n",
    "\n",
    "        if np.all(LEFT_SHOULDER) and np.all(LEFT_ELBOW) and np.all(LEFT_WRIST):\n",
    "            angle = func.calculate_angle(LEFT_SHOULDER, LEFT_ELBOW, LEFT_WRIST)\n",
    "            rula_score = func.get_rula_score(angle)\n",
    "            risk = func.get_risk_label(rula_score)\n",
    "\n",
    "            # Show on frame\n",
    "            cv2.putText(annotated_frame, f\"Angle: {int(angle)}Â°\", (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "            cv2.putText(annotated_frame, f\"RULA: {rula_score}\", (20, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0), 2)\n",
    "            cv2.putText(annotated_frame, f\"Risk: {risk}\", (20, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fb1f85",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "cv2.imshow(\"Ergonomic Posture Detection (YOLO-Pose)\", annotated_frame)\n",
    "# cv2.imwrite(cv2.imread(annotated_frame))\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pose Estimation Env",
   "language": "python",
   "name": "pose_estimation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
